{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Install necessary dependencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install opencv-python datasets tensorboard transformers torchvision\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu117 # for cuda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import all needed packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from torchvision.transforms import Normalize, Compose, RandomResizedCrop, RandomHorizontalFlip, RandomVerticalFlip, ToTensor, Resize, CenterCrop, GaussianBlur, RandomApply\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download dataset, remove unnecessary columns and add label 0 to real images and 1 to generated images.\n",
    "Then split the dataset into a train (80%), validation (10%) and test dataset (10%)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    # datasets\n",
    "    fake = load_dataset('poloclub/diffusiondb', '2m_random_10k', split='train', data_dir='./')\n",
    "    real = load_dataset('frgfm/imagenette', '320px', split='train+validation', data_dir='./')\n",
    "\n",
    "    # remove unnecessary columns\n",
    "    fake = fake.remove_columns(\n",
    "        ['prompt', 'seed', 'step', 'cfg', 'sampler', 'width', 'height', 'user_name', 'timestamp', 'image_nsfw',\n",
    "         'prompt_nsfw'])\n",
    "    real = real.remove_columns('label')\n",
    "\n",
    "    # add label column with 0 for real images and for 1 for generated images\n",
    "    fake = fake.map(lambda x: {'image': x['image'], 'label': 1})\n",
    "    real = real.map(lambda x: {'image': x['image'], 'label': 0})\n",
    "\n",
    "    # split fake dataset into train, validation and test sets\n",
    "    fake_train_testvalid = fake.train_test_split(test_size=0.2)\n",
    "    fake_test_valid = fake_train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "    # split real dataset into train, validation and test sets\n",
    "    real_train_testvalid = real.train_test_split(test_size=0.2)\n",
    "    real_test_valid = real_train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "    # combine fake and real datasets into single dataset for each split\n",
    "    train_dataset = concatenate_datasets([fake_train_testvalid['train'], real_train_testvalid['train']])\n",
    "    val_dataset = concatenate_datasets([fake_test_valid['train'], real_test_valid['train']])\n",
    "    test_dataset = concatenate_datasets([fake_test_valid['test'], real_test_valid['test']])\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform and preprocess data. Convert the images to pixel_values (tensors)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class HighpassFilter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, img):\n",
    "        kernel = np.array([[0.0, -1.0, 0.0],\n",
    "                           [-1.0, 5.0, -1.0],\n",
    "                           [0.0, -1.0, 0.0]])\n",
    "\n",
    "        kernel = kernel / (np.sum(kernel) if np.sum(kernel) != 0 else 1)\n",
    "\n",
    "        return cv.filter2D(np.array(img), -1, kernel)\n",
    "\n",
    "\n",
    "highpass = HighpassFilter()\n",
    "\n",
    "# transform/preprocess input\n",
    "train_transforms = Compose([\n",
    "    RandomResizedCrop(224),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    RandomApply([GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=0.5),\n",
    "    RandomApply([highpass], p=0.5),\n",
    "    ToTensor(),\n",
    "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "def transform_train(batch):\n",
    "    batch['pixel_values'] = [train_transforms(img.convert(\"RGB\")) for img in batch['image']]\n",
    "    del batch['image']\n",
    "    return batch\n",
    "\n",
    "\n",
    "def transform_val(batch):\n",
    "    batch['pixel_values'] = [val_transforms(img.convert(\"RGB\")) for img in batch['image']]\n",
    "    del batch['image']\n",
    "    return batch\n",
    "\n",
    "\n",
    "def transform_test(batch):\n",
    "    batch['pixel_values'] = [test_transforms(img.convert(\"RGB\")) for img in batch['image']]\n",
    "    del batch['image']\n",
    "    return batch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Help Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def iterate_dataloader(dataloader, device):\n",
    "    \"\"\"\n",
    "    Iterate through given dataloader and return image and label data.\n",
    "    \"\"\"\n",
    "    data = next(iter(dataloader))\n",
    "    images = data['pixel_values'].to(device)\n",
    "    labels = data['label'].to(device)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def inv_trans(image):\n",
    "    \"\"\"\n",
    "    Inverse transformations so that colors are in normal range.\n",
    "    \"\"\"\n",
    "    inverse_trans = Compose([\n",
    "        Normalize(mean=[0., 0., 0.], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
    "        Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.])])\n",
    "\n",
    "    return inverse_trans(image)\n",
    "\n",
    "\n",
    "def imshow(images, labels):\n",
    "    \"\"\"\n",
    "    Display images (batch) in one figure with labels.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    rows, columns = 2, 4\n",
    "\n",
    "    for i in range(8):\n",
    "        unnormalize_img = inv_trans(images[i]).cpu().numpy()\n",
    "        if int(labels[i]) == 0: label = 'real'\n",
    "        else: label = 'generated'\n",
    "\n",
    "        fig.add_subplot(rows, columns, i + 1)\n",
    "        plt.imshow(np.transpose(unnormalize_img, (1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        plt.title(label)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_best_loss():\n",
    "    \"\"\"\n",
    "    Get the best loss from previous runs if available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('./model_data/best_loss.pkl', 'rb') as file:\n",
    "            best_loss = pickle.load(file)\n",
    "        print(f'\\nbest loss: {best_loss}')\n",
    "    except FileNotFoundError:\n",
    "        best_loss = 42\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "\n",
    "def get_image_label(data, device):\n",
    "    \"\"\"\n",
    "    Get image and label data. Convert label data to the same nn output shape.\n",
    "    \"\"\"\n",
    "    image = data['pixel_values'].to(device)\n",
    "    label = data['label'].unsqueeze(1).float().to(device)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def save_model_with_best_loss(model, val_loss, device):\n",
    "    \"\"\"\n",
    "    Saves best model and best loss. Returns the best loss.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        traced = torch.jit.trace(model, torch.rand(1, 3, 224, 224).to(device))\n",
    "    torch.jit.save(traced, './model_data/resnet50.pth')\n",
    "\n",
    "    # save the best loss for next run\n",
    "    with open('model_data/best_loss_ds1.pkl', 'wb') as file: pickle.dump(val_loss, file)\n",
    "    print('model updated')\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def plot_loss(epoch_train_losses, epoch_test_losses, epochs):\n",
    "    epochs = list(range(1, epochs+1))\n",
    "    plt.plot(epochs, epoch_train_losses, 'r', label='Train Loss')\n",
    "    plt.plot(epochs, epoch_test_losses, 'g', label='Validation Loss')\n",
    "    plt.title('Train and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_acc(total_train_acc, total_val_acc, epochs):\n",
    "    epochs = list(range(1, epochs+1))\n",
    "    plt.plot(epochs, total_train_acc, 'r', label='Train Accuracy')\n",
    "    plt.plot(epochs, total_val_acc, 'g', label='Validation Accuracy')\n",
    "    plt.title('Train and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Validation of model. Model is saved when validation loss decreases.\n",
    "If model has not improved in a while, stop training early."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(n_epochs, model, train_loader, device, optimizer, loss_fn, val_loader, best_loss):\n",
    "    total_train_losses = []\n",
    "    total_val_losses = []\n",
    "    total_train_acc = []\n",
    "    total_val_acc = []\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_tolerance = 5\n",
    "\n",
    "    writer = SummaryWriter(\"model_data/logs\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # enter training mode\n",
    "        model.train()\n",
    "        print('Starting training...')\n",
    "\n",
    "        train_loss, total, correct = 0, 0, 0\n",
    "\n",
    "        for _, data in enumerate(tqdm(train_loader)):  # iterate over batches\n",
    "            image, label = get_image_label(data, device)\n",
    "\n",
    "            # zero gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # make prediction by giving model image\n",
    "                output = model(image)\n",
    "                probabilities = torch.sigmoid(output)\n",
    "\n",
    "                # compute loss\n",
    "                loss = loss_fn(output, label)\n",
    "\n",
    "            train_loss += loss / len(train_loader)\n",
    "\n",
    "            # calculate accuracy for training\n",
    "            predictions = probabilities > 0.5\n",
    "            correct += (predictions == label).sum().item()\n",
    "            total += predictions.size(0)\n",
    "\n",
    "            # perform backpropagation, update weights on model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_train_losses.append(train_loss)\n",
    "        print('\\nEpoch : {}, train loss : {}'.format(epoch + 1, train_loss))\n",
    "        writer.add_scalar('loss/training', train_loss, epoch)\n",
    "\n",
    "        # calculate training accuracy\n",
    "        train_acc = float(correct) / float(total) * 100\n",
    "        total_train_acc.append(train_acc)\n",
    "        print(\"Got {} / {} with training accuracy {}\".format(correct, total, train_acc))\n",
    "        writer.add_scalar('accuracy/training', train_acc, epoch)\n",
    "\n",
    "        # validation doesnt requires gradient\n",
    "        with torch.no_grad():\n",
    "            # model to eval mode\n",
    "            model.eval()\n",
    "\n",
    "            val_loss, total, correct = 0, 0, 0\n",
    "\n",
    "            for data in val_loader:\n",
    "                image, label = get_image_label(data, device)\n",
    "\n",
    "                # give model image to receive prediction\n",
    "                output = model(image)\n",
    "                probabilities = torch.sigmoid(output)\n",
    "\n",
    "                # calculate validation loss\n",
    "                v_loss = loss_fn(output, label)\n",
    "                val_loss += v_loss / len(val_loader)\n",
    "\n",
    "                # calculate accuracy for validation\n",
    "                predictions = probabilities > 0.5\n",
    "                correct += (predictions == label).sum().item()\n",
    "                total += predictions.size(0)\n",
    "\n",
    "            total_val_losses.append(val_loss)\n",
    "            print('Epoch : {}, val loss : {}'.format(epoch + 1, val_loss))\n",
    "            writer.add_scalar('loss/validation', val_loss, epoch)\n",
    "\n",
    "            # calculate validation accuracy\n",
    "            val_acc = float(correct) / float(total) * 100\n",
    "            total_val_acc.append(val_acc)\n",
    "            print(\"Got {} / {} with validation accuracy {}\".format(correct, total, val_acc))\n",
    "            writer.add_scalar('accuracy/validation', val_acc, epoch)\n",
    "\n",
    "            print('val_loss: ', val_loss)\n",
    "            print('best_loss ', best_loss)\n",
    "\n",
    "            # save best model\n",
    "            if val_loss <= best_loss: best_loss = save_model_with_best_loss(model, val_loss, device)\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss > best_loss:\n",
    "                early_stopping_counter += 1\n",
    "                print(f'early stopping counter {early_stopping_counter} / {early_stopping_tolerance}')\n",
    "\n",
    "            if early_stopping_counter == early_stopping_tolerance:\n",
    "                print(f'early stopping counter {early_stopping_counter} / {early_stopping_tolerance}')\n",
    "                print(\"Terminating: early stopping\")\n",
    "                break  # terminate training\n",
    "\n",
    "    return total_train_losses, total_val_losses, total_train_acc, total_val_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute Training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# used device for computing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device, 'will be used.')\n",
    "\n",
    "# get the dataset\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "\n",
    "# apply transforms and preprocessing to dataset\n",
    "train_dataset.set_transform(transform_train)\n",
    "val_dataset.set_transform(transform_val)\n",
    "test_dataset.set_transform(transform_test)\n",
    "\n",
    "# load data\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# showcase some training images\n",
    "images, labels = iterate_dataloader(train_loader, device)\n",
    "imshow(images, labels)\n",
    "\n",
    "# define model\n",
    "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# freeze all parameters\n",
    "for params in model.parameters(): params.requires_grad_ = False\n",
    "\n",
    "# add new final layer to customize model to become binary classifier\n",
    "nr_filters = model.fc.in_features  # number of input features of last layer\n",
    "model.fc = nn.Linear(nr_filters, 1)\n",
    "\n",
    "# load trained model if there is one\n",
    "try:\n",
    "    model = torch.jit.load('./model_data/resnet50.pth')\n",
    "    with torch.no_grad():\n",
    "        print(model)\n",
    "except ValueError:\n",
    "    print(\"There was no model to load.\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# loss; binary cross entropy with sigmoid, i.e. no need to use sigmoid in model\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-4)\n",
    "\n",
    "# get best loss\n",
    "best_loss = get_best_loss()\n",
    "\n",
    "# start training of model\n",
    "n_epochs = 0\n",
    "total_train_losses, total_val_losses, total_train_acc, total_val_acc =\\\n",
    "    train_model(n_epochs, model, train_loader, device, optimizer, loss_fn, val_loader, best_loss)\n",
    "\n",
    "print('evaluating model...')\n",
    "\n",
    "print('train loss: ', torch.Tensor(total_train_losses).cpu())\n",
    "print('val loss: ', torch.Tensor(total_val_losses).cpu())\n",
    "\n",
    "# plotting losses, accuracies\n",
    "duration = len(total_val_losses)\n",
    "plot_loss(torch.tensor(total_train_losses).cpu(), torch.tensor(total_val_losses).cpu(), duration)\n",
    "plot_acc(total_train_acc, total_val_acc, duration)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display tensorboard logs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir model_data/logs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}